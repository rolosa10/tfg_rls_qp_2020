{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ajustament de les diferents combinacions d'encoder-mètrica de similitud per determinar l'índex de similitud semàntica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importo llibreries i dependències"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import pickle\n",
    "from googletrans import Translator\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "import nltk\n",
    "import unidecode\n",
    "import re\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import metrics\n",
    "import time\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer('spanish')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Càrrega dels documents de la base de dades d'una categoria específica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LListo tots els tfg dins de la categoria mecanica de fluids. \n",
    "#Si volgués estudiar una altre categoria hauria de canviar el path.\n",
    "llista_text_documents_mecanica_fluids = []\n",
    "for file in glob.glob('../00Data/dataset_txt/Mecanica_fluids/*'):\n",
    "    with open(file,'r') as txt:\n",
    "        llista_text_documents_mecanica_fluids.append(txt.read())\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Càrrega del document plagiat a analitzar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Especificar el document a analitzar en el bucle inferior.\n",
    "llista_text_documents_plagiats = []\n",
    "for file in glob.glob('../00Data/plagiados_por_categorias/txt/Mecanica_fluids/Turbinas dinámicas y golpe de ariete.txt'):\n",
    "    with open(file,'r') as txt:\n",
    "        llista_text_documents_plagiats.append(txt.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessament del text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Funció que permet pre-processar el text dins de qualsevol tfg i separar-lo en oracions\n",
    "def clean_raw_text (documento):\n",
    "    text_sentences = []\n",
    "    text = str(documento).replace('\\n',' ')\n",
    "    text = nltk.sent_tokenize(text)\n",
    "    \n",
    "    for sentence in text:\n",
    "        words = []\n",
    "        sentence = sentence.lower()\n",
    "        word_tokens = nltk.word_tokenize(sentence)\n",
    "        filtered_text = \" \".join([w for w in word_tokens if not w in stopwords.words('spanish')])\n",
    "        stemmed_text = \" \".join(stemmer.stem(word) for word in nltk.word_tokenize(filtered_text))  \n",
    "        no_punctuaction_text = re.sub(r'[^\\w\\s]','',stemmed_text)\n",
    "        no_accents = unidecode.unidecode(no_punctuaction_text)\n",
    "        no_digits_text = re.sub('\\d', '', no_accents)\n",
    "        for word in nltk.word_tokenize(no_digits_text):\n",
    "            if len(word) >=2:\n",
    "                words.append(word)\n",
    "                                \n",
    "        no_digits_text = \" \".join(words)\n",
    "        clean1 = re.sub(' +', ' ',no_digits_text)\n",
    "        full_clean = clean1.strip()    \n",
    "        text_sentences.append(full_clean)\n",
    "        \n",
    "    text_sentences = [x for x in text_sentences if x !='']\n",
    "    return(text_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Es crea una llista de llista de llistes on per a cada document es llisten les diferents oracions pre-processades\n",
    "\n",
    "base_document_sentences = []\n",
    "for i in range (0,len(llista_text_documents_mecanica_fluids)):\n",
    "    base_document_sentences.append(clean_raw_text(llista_text_documents_mecanica_fluids[i]))\n",
    "    \n",
    "plag_document_sentences = []\n",
    "for i in range (0,len(llista_text_documents_plagiats)):\n",
    "    plag_document_sentences.append(clean_raw_text(llista_text_documents_plagiats[i]))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generar vocabulari"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92696\n"
     ]
    }
   ],
   "source": [
    "#Creo el vocabulari pel Word2Vec amb totes les paraules dels documents base\n",
    "\n",
    "vocab_words = []\n",
    "\n",
    "for tfg in llista_text_documents_mecanica_fluids:\n",
    "    tfg = tfg.replace('\\n',' ')\n",
    "    for parag in tfg.split(' '):\n",
    "        words = []\n",
    "        parag = parag.lower()\n",
    "        word_tokens = nltk.word_tokenize(parag)\n",
    "        filtered_text = \" \".join([w for w in word_tokens if not w in stopwords.words('spanish')])\n",
    "        stemmed_text = \" \".join(stemmer.stem(word) for word in nltk.word_tokenize(filtered_text))\n",
    "        no_punctuaction_text = re.sub(r'[^\\w\\s]','',stemmed_text)\n",
    "        no_accents = unidecode.unidecode(no_punctuaction_text)\n",
    "        no_digits_text = re.sub('\\d', '', no_accents)\n",
    "        for word in nltk.word_tokenize(no_digits_text):\n",
    "            if len(word) >=2:\n",
    "                words.append(word)\n",
    "                                \n",
    "        no_digits_text = \" \".join(words)\n",
    "        clean1 = re.sub(' +', ' ',no_digits_text)\n",
    "        full_clean = clean1.strip()    \n",
    "        \n",
    "        \n",
    "        \n",
    "        vocab_words.append(nltk.word_tokenize(full_clean))\n",
    "        \n",
    "\n",
    "    vocab_words\n",
    "\n",
    "vocab_words = [x for x in vocab_words if x !=[]]\n",
    "print(len(vocab_words))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF + cos similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sentences_in_one_list = []\n",
    "for document in base_document_sentences:\n",
    "    for sentence in document:\n",
    "        all_sentences_in_one_list.append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6899"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_sentences_in_one_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inicialitzo TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit_transform amb les sentences de base\n",
    "tfidf_vectorizer.fit(all_sentences_in_one_list)\n",
    "\n",
    "tf_idf_matrius_documents_base = []\n",
    "for document in base_document_sentences:\n",
    "    tf_idf_matrius_documents_base.append(tfidf_vectorizer.transform(document))\n",
    "\n",
    "\n",
    "\n",
    "#Transform les sentences de plagi\n",
    "tfidf_matrix_plag = tfidf_vectorizer.transform(plag_document_sentences[0])#Especifico l'unic document que hi ha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13.2 ms, sys: 2.13 ms, total: 15.3 ms\n",
      "Wall time: 15.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "similituds_per_cada_document = []\n",
    "for document in tf_idf_matrius_documents_base:\n",
    "    similituds_per_cada_document.append(cosine_similarity(document,tfidf_matrix_plag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "similitud_global_per_document = []\n",
    "for document in similituds_per_cada_document:\n",
    "    similitud_global_per_document.append(sum(document.max(axis=0))/len(document.max(axis=0)))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.2101534358616269,\n",
       " 0.16400976859556846,\n",
       " 0.07648039469877646,\n",
       " 0.2097877427776879,\n",
       " 0.2678788919295006,\n",
       " 0.10316638994815484,\n",
       " 0.2033653580661739,\n",
       " 0.19758900534534019,\n",
       " 0.20177721925011524,\n",
       " 0.1782031863029215,\n",
       " 0.15612839115754448,\n",
       " 0.186245235899371,\n",
       " 0.22758953708905946,\n",
       " 0.2336213877690274,\n",
       " 0.17369625792652288,\n",
       " 0.6464136971379421]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similitud_global_per_document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#L'index del document que té la similitud màxima. Podria printar els que tinguessin una similitud major al 35%\n",
    "similitud_global_per_document.index(max(similitud_global_per_document))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sha detectat plagi en el document analitzat\n",
      "El document amb títol : Estudio_fluidodinamico_de_un_agitador_de_turbina té una semblança del 64.6413697137942%\n"
     ]
    }
   ],
   "source": [
    "#EL DOCUMENT DEL QUAL S'HA FET EL PLAGI\n",
    "print('S''ha detectat plagi en el document analitzat')\n",
    "print('El document amb títol : '+str(glob.glob('../00Data/dataset_txt/Mecanica_fluids/*')[similitud_global_per_document.index(max(similitud_global_per_document))].split('/')[-1])+' té una semblança del '+str(max(similitud_global_per_document)*100)+'%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.18597948017449273"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#MITJANA DE SIMILITUDS ENTRE ELS DOCUMENTS SENSE CONSIDERAR LA MÀXIMA SIMILITUD:\n",
    "sum(similitud_global_per_document[:-1])/len(similitud_global_per_document[:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WORD2VEC "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 1e+03 ns, total: 3 µs\n",
      "Wall time: 5.01 µs\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "%time\n",
    "w2v_model = Word2Vec(vocab_words,\n",
    "                     size=50,\n",
    "                     min_count=1, #MIN COUNT EN 1. SINO TINC PROBLEMES A L'HORA DE COMPUTAR COSINE SIMILARITY\n",
    "                     window=2,\n",
    "                     workers=4) \n",
    "\n",
    "w2v_model.init_sims(replace=True)  # Normalizes the vectors in the word2vec class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7534"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(w2v_model.wv.vocab) #Vocabulari del word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Descarrego el model\n",
    "pkl_filename = \"w2v_semantic_text_similarity_mecanica_fluids.pkl\"\n",
    "with open('../03Models/w2v_models_for_compute_similarities/'+pkl_filename, 'wb') as file:\n",
    "    pickle.dump(w2v_model, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2vec + cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creo una llista de llistes on tinc les diferents frases tokenitzades de cada document. [[[]]]\n",
    "base_documents_frases_tokenitzades = []\n",
    "for i in range (0,len(base_document_sentences)):\n",
    "    frases_tokenitzades = []\n",
    "    for sentence in base_document_sentences[i]:\n",
    "        frases_tokenitzades.append(nltk.word_tokenize(sentence))#Tokenitzo cada frase, obtenint una llista de paraules per a cada frase\n",
    "    base_documents_frases_tokenitzades.append(frases_tokenitzades)\n",
    "    \n",
    "    \n",
    "    \n",
    "#Faig el mateix amb el document plagiat\n",
    "plag_document_frases_tokenitzades = []\n",
    "for sentence in plag_document_sentences[0]: \n",
    "    plag_document_frases_tokenitzades.append(nltk.word_tokenize(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filtro les paraules dels documents base de cada frase segons si pertanyen al vocabulari del word2vec\n",
    "base_filtered_document_sentence_global = []\n",
    "for document in base_documents_frases_tokenitzades: \n",
    "    filtered_sentence_global=[] \n",
    "    for sentence in document: \n",
    "        filtered_sentence_global.append([word for word in sentence if word in w2v_model.wv.vocab])\n",
    "    base_filtered_document_sentence_global.append(filtered_sentence_global)\n",
    "    \n",
    "    \n",
    "#Faig el mateix amb el document plagiat\n",
    "plag_filtered_document_sentence = []\n",
    "for sentence in plag_document_frases_tokenitzades:\n",
    "    plag_filtered_document_sentence.append([word for word in sentence if word in w2v_model.wv.vocab])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "#L'índex son els documents, a cada row hi ha una llista de les diferents frases tokentizades.\n",
    "df_base = pd.DataFrame({'Filtered_Sentence': base_filtered_document_sentence_global})\n",
    "df_plag = pd.DataFrame({'Filtered Sentence': plag_filtered_document_sentence})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transformo a vectors les paraules de cada frase de cada document a partir del wor2vec.\n",
    "#No ha estat necessari filtrar les paraules perquè totes ja formen part del vocabulari. Si no fos així, hauria de filtrar-les\n",
    "base_document_sentence_vectors = []\n",
    "for i in range (0,len(base_filtered_document_sentence_global)):\n",
    "    frase_vectorizada = []\n",
    "    for sentence in base_filtered_document_sentence_global[i]:\n",
    "        mean = []\n",
    "        for word in sentence:\n",
    "            mean.append(w2v_model.wv.get_vector(word))\n",
    "        mean = np.array(mean).mean(axis=0)\n",
    "        frase_vectorizada.append(mean)\n",
    "    base_document_sentence_vectors.append(frase_vectorizada)\n",
    "    \n",
    "    \n",
    "#plagi\n",
    "plag_document_sentence_vectors = []\n",
    "for sentence in plag_filtered_document_sentence:\n",
    "    mean = []\n",
    "    for word in sentence:\n",
    "        mean.append(w2v_model.wv.get_vector(word))\n",
    "    mean = np.array(mean).mean(axis=0)\n",
    "    plag_document_sentence_vectors.append(mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transformo els vectors d'una dimensió a dues dimensions per tal de poder aplicar posteriorment cosine similarity\n",
    "base_document_sentence_vectors_good_format = []\n",
    "for document in base_document_sentence_vectors:\n",
    "    vectorizado_bueno = []\n",
    "    for vector in document:\n",
    "        vectorizado_bueno.append([vector])\n",
    "    base_document_sentence_vectors_good_format.append(vectorizado_bueno)\n",
    "    \n",
    "#El mateix pel document plagiat\n",
    "plag_document_sentence_vectors_good_format = []\n",
    "for vector in plag_document_sentence_vectors:\n",
    "    plag_document_sentence_vectors_good_format.append([vector])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 16.2 s, sys: 71.2 ms, total: 16.3 s\n",
      "Wall time: 16.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "similarities_between_each_vector_plag_each_vector_base = []\n",
    "for i in range (0,len(base_document_sentence_vectors_good_format)):\n",
    "    each_document = []\n",
    "    for vector2 in plag_document_sentence_vectors_good_format:\n",
    "        each_plag_sentence = []\n",
    "        for vector1 in base_document_sentence_vectors_good_format[i]:\n",
    "            each_plag_sentence.append(cosine_similarity(vector2,vector1))\n",
    "        each_document.append(each_plag_sentence)\n",
    "    similarities_between_each_vector_plag_each_vector_base.append(each_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(similarities_between_each_vector_plag_each_vector_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(similarities_between_each_vector_plag_each_vector_base[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "541"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(similarities_between_each_vector_plag_each_vector_base[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "#La llista similarities_between_each_vector_plag_each_vector_base es una llista que conté 16 elements (documents):\n",
    "#Per a cada document hi ha una llista de 14 elements. \n",
    "#Cadascun dels 14 elements es correspon a un dels vectors del document plagiat. \n",
    "#Dins de cadascun dels 14 elements hi ha 541 similituds.\n",
    "#S'ha comparat cada vector del document plagiat amb cada vector del document base.\n",
    "#L'objectiu es que per a cada document extreure la similitud màxima de cadascun dels 14 elements \n",
    "#i calcular l'index de simlitud global."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_similarities_sentence_document = []\n",
    "for i in range (0,len(similarities_between_each_vector_plag_each_vector_base)):\n",
    "    each_document = []\n",
    "    for each_vector in similarities_between_each_vector_plag_each_vector_base[i]:\n",
    "        each_document.append(max(each_vector))\n",
    "    best_similarities_sentence_document.append(each_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0.48598108]], dtype=float32),\n",
       " array([[0.40634847]], dtype=float32),\n",
       " array([[0.314605]], dtype=float32),\n",
       " array([[0.43441653]], dtype=float32),\n",
       " array([[0.46903542]], dtype=float32),\n",
       " array([[0.35735634]], dtype=float32),\n",
       " array([[0.4866169]], dtype=float32),\n",
       " array([[0.4273612]], dtype=float32),\n",
       " array([[0.4642219]], dtype=float32),\n",
       " array([[0.4320094]], dtype=float32),\n",
       " array([[0.4183214]], dtype=float32),\n",
       " array([[0.4368577]], dtype=float32),\n",
       " array([[0.52176607]], dtype=float32),\n",
       " array([[0.4840319]], dtype=float32),\n",
       " array([[0.43751302]], dtype=float32),\n",
       " array([[0.7719879]], dtype=float32)]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_entre_documents_w2v = []\n",
    "for each_document in best_similarities_sentence_document:\n",
    "    index_entre_documents_w2v.append(sum(each_document)/len(each_document))\n",
    "    \n",
    "index_entre_documents_w2v  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.7719879]], dtype=float32)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Índex de similitud entre documents\n",
    "max(index_entre_documents_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Estudio_fluidodinamico_de_un_agitador_de_turbina'"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glob.glob('../00Data/dataset_txt/Mecanica_fluids/*')[index_entre_documents_w2v.index(max(index_entre_documents_w2v))].split('/')[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.43842947]], dtype=float32)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Mitjana de similituds entre els documents sense considerar la màxima similitud\n",
    "sum(index_entre_documents_w2v[:-1])/len(index_entre_documents_w2v[:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2vec embeddings + word mover's distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyemd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rogerlopezsantalo/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `wmdistance` (Method will be removed in 4.0.0, use self.wv.wmdistance() instead).\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 23s, sys: 693 ms, total: 4min 23s\n",
      "Wall time: 4min 25s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "wmdist_between_each_vector_plag_each_vector_base = []\n",
    "\n",
    "for i in range (0,len(base_filtered_document_sentence_global)):\n",
    "    each_document = [] \n",
    "    for sentence1 in plag_filtered_document_sentence:\n",
    "        each_plag_sentence = []\n",
    "        for sentence2 in base_filtered_document_sentence_global[i]:\n",
    "            each_plag_sentence.append(w2v_model.wmdistance(sentence2,sentence1))\n",
    "                                      \n",
    "        each_document.append(each_plag_sentence)\n",
    "                                      \n",
    "    wmdist_between_each_vector_plag_each_vector_base.append(each_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wmdist_between_each_vector_plag_each_vector_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wmdist_between_each_vector_plag_each_vector_base[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "541"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wmdist_between_each_vector_plag_each_vector_base[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "less_distance_sentence_document = []\n",
    "for i in range (0,len(wmdist_between_each_vector_plag_each_vector_base)):\n",
    "    each_document = []\n",
    "    for each_vector in wmdist_between_each_vector_plag_each_vector_base[i]:\n",
    "        each_document.append(min(each_vector))\n",
    "    less_distance_sentence_document.append(each_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9428449303501133,\n",
       " 1.0771786720228143,\n",
       " 1.05127246715718,\n",
       " 0.859252649579193,\n",
       " 1.0986147916534428,\n",
       " 1.0875597113012556,\n",
       " 0.9730258332700894,\n",
       " 1.0505522011919317,\n",
       " 0.9993910348652768,\n",
       " 0.9489173508866168,\n",
       " 1.0092065774347254,\n",
       " 1.046650517796031,\n",
       " 0.9698651883140696,\n",
       " 0.8989752608608694]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "less_distance_sentence_document[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0009505133345435,\n",
       " 1.055623165602435,\n",
       " 1.1427019837248673,\n",
       " 1.0356003308711226,\n",
       " 1.011058408232542,\n",
       " 1.1044433624207233,\n",
       " 1.0394452348479715,\n",
       " 1.0289421062446322,\n",
       " 1.020307430692151,\n",
       " 1.031916893890839,\n",
       " 1.080745489271955,\n",
       " 1.0182708095978548,\n",
       " 0.9915994734672594,\n",
       " 1.0369422084411264,\n",
       " 1.0481899604027833,\n",
       " 0.44136889358612336]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distance_entre_documents_w2v = []\n",
    "for each_document in less_distance_sentence_document:\n",
    "    distance_entre_documents_w2v.append(sum(each_document)/len(each_document))\n",
    "    \n",
    "distance_entre_documents_w2v  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.44136889358612336"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Distància mínima entre documents\n",
    "min(distance_entre_documents_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Estudio_fluidodinamico_de_un_agitador_de_turbina'"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glob.glob('../00Data/dataset_txt/Mecanica_fluids/*')[distance_entre_documents_w2v.index(min(distance_entre_documents_w2v))].split('/')[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0431158247361871"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(distance_entre_documents_w2v[:-1])/len(distance_entre_documents_w2v[:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2vec + Smooth inverse frequency + cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "plag_filtered_document_sentence_no_tokenized = []\n",
    "for sentence in plag_filtered_document_sentence:\n",
    "    plag_filtered_document_sentence_no_tokenized.append(' '.join(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_filtered_document_sentence_no_tokenized = []\n",
    "for document in base_filtered_document_sentence_global:\n",
    "    listado_documentos = []\n",
    "    for sentence in document:\n",
    "        listado_documentos.append(' '.join(sentence))\n",
    "    base_filtered_document_sentence_no_tokenized.append(listado_documentos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_frequency(word_text):\n",
    "    return 0.0001  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "plag_sif_sentences_vectorized = []\n",
    "a: float=1e-3\n",
    "for sentence in plag_filtered_document_sentence_no_tokenized:\n",
    "    vs = np.zeros(50)\n",
    "    sentence_length = len(sentence)\n",
    "    for word in nltk.word_tokenize(sentence):\n",
    "        a_value = a/(a+get_word_frequency(word))\n",
    "        vs = np.add(vs, np.multiply(a_value,w2v_model.wv.get_vector(word)))\n",
    "    vs = np. divide(vs,sentence_length)\n",
    "    plag_sif_sentences_vectorized.append(vs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_sif_document_sentences_vectorized = []\n",
    "a: float=1e-3\n",
    "    \n",
    "for document in base_filtered_document_sentence_no_tokenized:\n",
    "    lista_documents = []\n",
    "    for sentence in document:\n",
    "        vs = np.zeros(50)\n",
    "        sentence_length = len(sentence)\n",
    "        for word in nltk.word_tokenize(sentence):\n",
    "            a_value = a/(a+get_word_frequency(word))\n",
    "            vs = np.add(vs, np.multiply(a_value,w2v_model.wv.get_vector(word)))\n",
    "        vs = np. divide(vs,sentence_length)\n",
    "        lista_documents.append(vs)\n",
    "    base_sif_document_sentences_vectorized.append(lista_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converteixo els vectors generats per a cada frase a vectors de 2D per tal d'aplicar cosine similarity entre ells.\n",
    "plag_sif_sentences_vectorized_good_format = []\n",
    "for vector in plag_sif_sentences_vectorized:\n",
    "    plag_sif_sentences_vectorized_good_format.append([vector])\n",
    "    \n",
    "base_sif_document_sentences_vectorized_good_format = []\n",
    "for document in base_sif_document_sentences_vectorized:\n",
    "    list_document = []\n",
    "    for vector_sentence in document:\n",
    "        list_document.append([vector_sentence])\n",
    "    base_sif_document_sentences_vectorized_good_format.append(list_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17.6 s, sys: 213 ms, total: 17.8 s\n",
      "Wall time: 18.5 s\n"
     ]
    }
   ],
   "source": [
    "#Computo similituds entre vectors\n",
    "%%time\n",
    "similarities_between_vectors_base_plag_sif = []\n",
    "\n",
    "for i in range (0,len(base_sif_document_sentences_vectorized_good_format)):\n",
    "    each_document = []\n",
    "    for vector2 in plag_sif_sentences_vectorized_good_format:\n",
    "        each_plag_sentence = []\n",
    "        for vector1 in base_sif_document_sentences_vectorized_good_format[i]:\n",
    "            each_plag_sentence.append(cosine_similarity(vector2,vector1))\n",
    "        each_document.append(each_plag_sentence)\n",
    "    similarities_between_vectors_base_plag_sif.append(each_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ara la llista best_similarities_sentence_document_sif és una llista dels 16 documents de la base de dades que contenen\n",
    "#Les màximes similituds amb cada frase del document plagiat (14 similituds)\n",
    "best_similarities_sentence_document_sif = []\n",
    "for i in range (0,len(similarities_between_vectors_base_plag_sif)):\n",
    "    each_document = []\n",
    "    for each_vector in similarities_between_vectors_base_plag_sif[i]:\n",
    "        each_document.append(max(each_vector))\n",
    "    best_similarities_sentence_document_sif.append(each_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0.48598111]]),\n",
       " array([[0.40634844]]),\n",
       " array([[0.31460501]]),\n",
       " array([[0.43441651]]),\n",
       " array([[0.46903542]]),\n",
       " array([[0.35735636]]),\n",
       " array([[0.48661694]]),\n",
       " array([[0.42736121]]),\n",
       " array([[0.46422189]]),\n",
       " array([[0.43200934]]),\n",
       " array([[0.41832141]]),\n",
       " array([[0.4368577]]),\n",
       " array([[0.52176605]]),\n",
       " array([[0.48403186]]),\n",
       " array([[0.43751297]]),\n",
       " array([[0.77198786]])]"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Calculo l'índex de simliitud per a cada document amb el document plagiat\n",
    "index_entre_documents_w2v_sif = []\n",
    "for each_document in best_similarities_sentence_document_sif:\n",
    "    index_entre_documents_w2v_sif.append(sum(each_document)/len(each_document))\n",
    "    \n",
    "index_entre_documents_w2v_sif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.77198786]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Estudio_fluidodinamico_de_un_agitador_de_turbina'"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Document amb màxima similitud\n",
    "print(max(index_entre_documents_w2v_sif))\n",
    "glob.glob('../00Data/dataset_txt/Mecanica_fluids/*')[index_entre_documents_w2v_sif.index(max(index_entre_documents_w2v_sif))].split('/')[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Weighting amb SIF i fent mean, els resultats son pràcticament els mateixos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.43842948]])"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(index_entre_documents_w2v_sif[:-1])/len(index_entre_documents_w2v_sif[:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UNIVERSAL SENTENCE ENCODER - MULTILINGUAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text #ERA EL QUE NECESSITAVA!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.21 s, sys: 723 ms, total: 2.93 s\n",
      "Wall time: 3.51 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder-multilingual/3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Faig embed de la llista de sentences de tot un document:\n",
    "base_document_sentences_embeddings_use = []\n",
    "for i in range(0,len(base_document_sentences)):\n",
    "    base_document_sentences_embeddings_use.append(embed(base_document_sentences[i]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculo el embeddings de cada frase d'un document en una mateixa matriu que sera una 14x512\n",
    "plag_document_sentences_embeddings_use = embed(plag_document_sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([541, 512])"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_document_sentences_embeddings_use[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 37.7 ms, sys: 5.76 ms, total: 43.4 ms\n",
      "Wall time: 23.1 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Calculo la similitud de la matriu d'embeddings de cada document base amb la matriu d'embeddings de la matriu plagi\n",
    "#Adjunto els resultats a similarities_between_base_plag\n",
    "similarities_between_base_plag = []\n",
    "for document_tensor in base_document_sentences_embeddings_use:\n",
    "    similarities_between_base_plag.append(cosine_similarity(document_tensor,plag_document_sentences_embeddings_use))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_similarities_between_base_plag = []\n",
    "for document in similarities_between_base_plag:\n",
    "    best_similarities_between_base_plag.append(document.max(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_index_use_between_base_plag = []\n",
    "for i in range (0,len(best_similarities_between_base_plag)):\n",
    "    similarity_index_use_between_base_plag.append(sum(best_similarities_between_base_plag[i]/len(best_similarities_between_base_plag[i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5140185728669167,\n",
       " 0.4290044233202934,\n",
       " 0.36235099472105503,\n",
       " 0.45256517082452774,\n",
       " 0.5055232439190149,\n",
       " 0.3982669096440077,\n",
       " 0.4534240011125803,\n",
       " 0.4882507435977459,\n",
       " 0.503564853221178,\n",
       " 0.4508955143392086,\n",
       " 0.44092260859906673,\n",
       " 0.48860074020922184,\n",
       " 0.5004921369254589,\n",
       " 0.49465914256870747,\n",
       " 0.4727472700178623,\n",
       " 0.7707100156694651]"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Printar totes les similituds i la maxima tambe\n",
    "similarity_index_use_between_base_plag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7707100156694651\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Estudio_fluidodinamico_de_un_agitador_de_turbina'"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(max(similarity_index_use_between_base_plag))\n",
    "glob.glob('../00Data/dataset_txt/Mecanica_fluids/*')[similarity_index_use_between_base_plag.index(max(similarity_index_use_between_base_plag))].split('/')[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.463685755059123"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Mitjana de similituds sense tenir en compte la màxima\n",
    "sum(similarity_index_use_between_base_plag[:-1])/len(similarity_index_use_between_base_plag[:-1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
