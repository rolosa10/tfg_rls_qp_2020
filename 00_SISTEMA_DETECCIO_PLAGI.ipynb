{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANEX A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuació, es mostren els diferents scripts que s'han utilitzat per a la creació del sistema de detecció de plagi. L'ordre dels scripts que es mostren és el següent:\n",
    "\n",
    "1. Sistema de detecció de plagi mitjançant intel·ligència artificial\n",
    "2. Models de classificació de documents en múltiples categories\n",
    "3. Ajustament de les diferents combinacions d'encoder-mètrica de similitud per determinar l'índex de similitud semàntica\n",
    "4. Distribució de freqüència de similitud entre oracions\n",
    "\n",
    "Respecte l'script 3, només es mostra l'execució d'aquest en un dels 24 documents plagiats generats. En concret, en el Doc1.0, corresponent a la categoria mecànica de fluids. Aquest script pot ser extrapolat als 23 documents restants.\n",
    "\n",
    "En el cas de l'script 4, només es mostra l'execució d'aquest per a la categoria electrònica. El codi és extrapolable a les demés categories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sistema de detecció de plagi mitjançant intel·ligència artificial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importació de les biblioteques/dependències necessàries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "import nltk\n",
    "import unidecode\n",
    "import re\n",
    "from sklearn import metrics\n",
    "import time\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import unidecode\n",
    "from google.cloud import vision\n",
    "import io\n",
    "import shutil\n",
    "from google.cloud import storage\n",
    "import os\n",
    "import json\n",
    "import docx2txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = '/Users/rogerlopezsantalo/Desktop/TFG/97_Keys/proyecto_personal/plagio-documentos-266509-4a5fc329ded0.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer('spanish')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ESPECIFICAR PATH DEL DOCUMENT A ANALITZAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../06FUNCIONAMENT_SISTEMA/Arxiu_analitzar/Curvas características de una bomba hidráulica centrifuga.jpg'\n",
    "path_doc_analitzar = path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Càrrega de l'encoder i el model de classificació"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encoder: CountVectorizer Binary\n",
    "with open('../03Models/classificacio/cv_binary.pkl', 'rb') as file:\n",
    "    pickle_feature = pickle.load(file)\n",
    "    \n",
    "#Model de classificació\n",
    "with open('../03Models/classificacio/model_classificacio_mnb.pkl', 'rb') as file:\n",
    "    pickle_model = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicció de la categoria del document a analitzar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La categoria que prediu el model de classificació per al document amb títol: Curvas características de una bomba hidráulica centrifuga es: \u001b[1mMecanica_fluids\u001b[0;0m\n"
     ]
    }
   ],
   "source": [
    "#Funció que permet executar la predicció a partir de la combinació encoder+model classificació (Cv binary + MLR)\n",
    "def prediccion(titulo):\n",
    "    titulo = titulo.split('/')[-1].split('.')[0]\n",
    "    titulo = titulo.replace('_',' ')\n",
    "    \n",
    "    word_tokens = nltk.word_tokenize(titulo)\n",
    "    text_without_stopwords= \" \".join([w for w in word_tokens if not w in stopwords.words('spanish')])\n",
    "    stemmed_text = \" \".join(stemmer.stem(word) for word in nltk.word_tokenize(text_without_stopwords))\n",
    "    no_punctuaction_text = re.sub(r'[^\\w\\s]','',stemmed_text)\n",
    "    #no_punctuaction_text = unidecode.unidecode(stemmed_text)\n",
    "    no_digits_text = re.sub('\\d', '', no_punctuaction_text)\n",
    "    X = pickle_feature.transform(pd.Series(no_digits_text))\n",
    "    return(pickle_model.predict(X))\n",
    "\n",
    "start = \"\\033[1m\"\n",
    "end = \"\\033[0;0m\"\n",
    "\n",
    "label = prediccion(path)\n",
    "print('La categoria que prediu el model de classificació per al document amb títol: '+str(path.split('/')[-1].split('.')[0])+' es: '+start+label[0]+end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Càlcul de similitud semàntica textual del document a analitzar amb els demés documents de la base de dades que tenen assignada la mateixa categoria. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Funció que permet pre-processar el text dins de qualsevol tfg i separar-lo en sentences\n",
    "def clean_raw_text (documento):\n",
    "    text_sentences = []\n",
    "    text = str(documento).replace('\\n',' ')\n",
    "    text = nltk.sent_tokenize(text)\n",
    "    \n",
    "    for sentence in text:\n",
    "        words = []\n",
    "        sentence = sentence.lower()\n",
    "        word_tokens = nltk.word_tokenize(sentence)\n",
    "        filtered_text = \" \".join([w for w in word_tokens if not w in stopwords.words('spanish')])\n",
    "        stemmed_text = \" \".join(stemmer.stem(word) for word in nltk.word_tokenize(filtered_text))  \n",
    "        no_punctuaction_text = re.sub(r'[^\\w\\s]','',stemmed_text)\n",
    "        no_accents = unidecode.unidecode(no_punctuaction_text)\n",
    "        no_digits_text = re.sub('\\d', '', no_accents)\n",
    "        for word in nltk.word_tokenize(no_digits_text):\n",
    "            if len(word) >=2:\n",
    "                words.append(word)\n",
    "                                \n",
    "        no_digits_text = \" \".join(words)\n",
    "        clean1 = re.sub(' +', ' ',no_digits_text)\n",
    "        full_clean = clean1.strip()    \n",
    "        text_sentences.append(full_clean)\n",
    "        \n",
    "    text_sentences = [x for x in text_sentences if x !='']\n",
    "    return(text_sentences)\n",
    "\n",
    "#Funció que permet descarregar un objecte de Google Storage\n",
    "def download_blob(bucket_name, source_blob_name, destination_file_name):\n",
    "        \"\"\"Downloads a blob from the bucket.\"\"\"\n",
    "        # bucket_name = \"your-bucket-name\"\n",
    "        # source_blob_name = \"storage-object-name\"\n",
    "        # destination_file_name = \"local/path/to/file\"\n",
    "\n",
    "        storage_client = storage.Client()\n",
    "\n",
    "        bucket = storage_client.bucket(bucket_name)\n",
    "        blob = bucket.blob(source_blob_name)\n",
    "        blob.download_to_filename(destination_file_name)\n",
    "\n",
    "        print(\n",
    "            \"Descarregant arxiu: {} localment. \\n\".format(\n",
    "                source_blob_name, destination_file_name\n",
    "            )\n",
    "        )\n",
    "\n",
    "#Funció que permet carregar un objecte a Google Storage        \n",
    "def upload_blob(bucket_name, source_file_name, destination_blob_name):\n",
    "        \"\"\"Uploads a file to the bucket.\"\"\"\n",
    "        # bucket_name = \"your-bucket-name\"\n",
    "        # source_file_name = \"local/path/to/file\"\n",
    "        # destination_blob_name = \"storage-object-name\"\n",
    "\n",
    "        storage_client = storage.Client(project='plagio-documentos-266509')\n",
    "        bucket = storage_client.bucket(bucket_name)\n",
    "        blob = bucket.blob(destination_blob_name)\n",
    "\n",
    "        blob.upload_from_filename(source_file_name)\n",
    "\n",
    "        print(\n",
    "            \"Arxiu {} pujat a storage amb el nom: {}. \\n\".format(\n",
    "                source_file_name, destination_blob_name\n",
    "            )\n",
    "        )\n",
    "\n",
    "#Funció que permet eliminar un objecte de Google Storage\n",
    "def delete_blob(bucket_name, blob_name):\n",
    "    \"\"\"Deletes a blob from the bucket.\"\"\"\n",
    "    # bucket_name = \"your-bucket-name\"\n",
    "    # blob_name = \"your-object-name\"\n",
    "\n",
    "    storage_client = storage.Client()\n",
    "\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blob = bucket.blob(blob_name)\n",
    "    blob.delete()\n",
    "\n",
    "    print(\"Arxiu {} eliminat.\\n\".format(blob_name))\n",
    "\n",
    "#FUNCIÓ COMPARACIÓ DOCUMENT A ANALITZAR VS DOCUMENTS DE LA MATEIXA CATEGORIA ASSIGNADA\n",
    "def comparacio_doc_analitzar_vs_base_dades(path_doc_analitzar,categoria):\n",
    "    \n",
    "    \n",
    "    #PELS DOCUMENTS A ANALITZAR .PDF\n",
    "    if path_doc_analitzar.split('/')[-1].split('.')[-1] == 'pdf':\n",
    "\n",
    "        client = storage.Client(project='plagio-documentos-266509')\n",
    "        upload_blob('arxius_analitzar','../06FUNCIONAMENT_SISTEMA/Arxiu_analitzar/'+path_doc_analitzar.split('/')[-1],path_doc_analitzar.split('/')[-1])\n",
    "        doc_extreure = []\n",
    "        for blob in client.list_blobs('arxius_analitzar'): #!!! Canviar el prefix per 'dataset_pdf' ja que 'original_dataset' es el de proves\n",
    "            doc_extreure.append(blob.name)\n",
    "        \n",
    "        \n",
    "        #Crea un o varis json a la carpeta 'dataset_json/[Sub_carpeta_especifica]' dins del bucket 'dataset_tfg' per a cada document. \n",
    "        #El json conté tot el text del document\n",
    "\n",
    "        for file in doc_extreure:\n",
    "            gcs_source_uri = 'gs://arxius_analitzar/'+file\n",
    "            gcs_destination_uri = 'gs://json_analitzar/'+file\n",
    "\n",
    "            \"\"\"OCR with PDF/TIFF as source files on GCS\"\"\"\n",
    "            # Supported mime_types are: 'application/pdf' and 'image/tiff' in case of DOCUMENT_TEXT_DETECTION\n",
    "         \n",
    "            mime_type = 'application/pdf'\n",
    "\n",
    "            # How many pages should be grouped into each json output file.\n",
    "            batch_size = 100 ## Número de pàgines que seran agrupades en un mateix json file. El màxim és 100\n",
    "            client = vision.ImageAnnotatorClient()\n",
    "\n",
    "            feature = vision.types.Feature(type=vision.enums.Feature.Type.DOCUMENT_TEXT_DETECTION)\n",
    "\n",
    "            gcs_source = vision.types.GcsSource(uri=gcs_source_uri)\n",
    "            input_config = vision.types.InputConfig(gcs_source=gcs_source, mime_type=mime_type)\n",
    "\n",
    "            gcs_destination = vision.types.GcsDestination(uri=gcs_destination_uri)\n",
    "            output_config = vision.types.OutputConfig(gcs_destination=gcs_destination, batch_size=batch_size)\n",
    "\n",
    "            async_request = vision.types.AsyncAnnotateFileRequest(features=[feature], input_config=input_config,output_config=output_config)\n",
    "\n",
    "            operation = client.async_batch_annotate_files(requests=[async_request])\n",
    "\n",
    "            print('Extraient text del document mitjantçant Google API VISION. \\n')\n",
    "            operation.result(timeout=600)\n",
    "            \n",
    "            \n",
    "        #Descarrego arxiu Json localment\n",
    "        nom_json_descarregar = []\n",
    "        storage_client = storage.Client()\n",
    "        for blob in storage_client.list_blobs('json_analitzar'):\n",
    "            nom_json_descarregar.append(blob.name)\n",
    "            \n",
    "        download_blob('json_analitzar',nom_json_descarregar[0],'../06FUNCIONAMENT_SISTEMA/json_analitzar/'+nom_json_descarregar[0].split('.pdf')[0]+'.json')\n",
    "        \n",
    "        #Elimino arxiu .pdf i .json del cloud  \n",
    "        nom_pdf_eliminar = []\n",
    "        storage_client = storage.Client()\n",
    "        for blob in storage_client.list_blobs('arxius_analitzar'):\n",
    "            nom_pdf_eliminar.append(blob.name)\n",
    "            \n",
    "        delete_blob('arxius_analitzar',nom_pdf_eliminar[0])\n",
    "        delete_blob('json_analitzar',nom_json_descarregar[0])\n",
    "            \n",
    "        #Extrec el text de l'arxiu json a un arxiu txt generat localment \n",
    "        json_doc_list = (glob.glob(\"../06FUNCIONAMENT_SISTEMA/json_analitzar/*\"))  \n",
    "        \n",
    "        for file in json_doc_list:\n",
    "            texto = file.split('/')[-1]\n",
    "\n",
    "            with open(file,'r') as json_file:\n",
    "                values = json.load(json_file)\n",
    "\n",
    "\n",
    "            with open('../06FUNCIONAMENT_SISTEMA/txt_analitzar/'+file.split('/')[-1].split('.json')[0]+'.txt','w+') as f:\n",
    "                try:   \n",
    "                    for i in range(0,len(values['responses'])):\n",
    "                        try:\n",
    "                            f.write(values['responses'][i]['fullTextAnnotation']['text'])\n",
    "                        except KeyError as e:\n",
    "                            if(e.args[0] != 'fullTextAnnotation'):\n",
    "                                print(e)\n",
    "                                raise(e)   \n",
    "                except:\n",
    "                    print(file)\n",
    "        print('Text extret a un arxiu .txt \\n')\n",
    "        \n",
    "        \n",
    "    #PELS DOCUMENTS .DOCX\n",
    "    elif path_doc_analitzar.split('/')[-1].split('.')[-1] == 'docx':\n",
    "        result = docx2txt.process(path_doc_analitzar)\n",
    "        result = result.replace('\\n',' ')\n",
    "        result = \" \".join(result.split())\n",
    "        print('Extraient el text del document a un arxiu .txt \\n')\n",
    "        with open('../06FUNCIONAMENT_SISTEMA/txt_analitzar/'+path_doc_analitzar.split('/')[-1].split('.')[0]+'.txt','w+') as file:\n",
    "            file.write(result)\n",
    "        print('Text extret a un arxiu .txt \\n')\n",
    "            \n",
    "            \n",
    "    #PELS DOCUMENTS .JPEG, .PNG8,.PNG24,.GIF\n",
    "    else:\n",
    "        print('Extraient el text a un arxiu .txt \\n')\n",
    "        client = vision.ImageAnnotatorClient()\n",
    "        with io.open(path_doc_analitzar, 'rb') as image_file:\n",
    "            content = image_file.read()\n",
    "        image = vision.types.Image(content=content)\n",
    "        response = client.text_detection(image=image)\n",
    "        \n",
    "        \n",
    "        with open('../06FUNCIONAMENT_SISTEMA/txt_analitzar/'+path_doc_analitzar.split('/')[-1].split('.')[0]+'.txt','w+') as file:\n",
    "            file.write(response.text_annotations[0].description.replace('\\n',' '))\n",
    "        print('Text extret a un arxiu .txt \\n')\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "   \n",
    "    #CÀLCUL DE L'ÍNDEX DE SIMILITUD SEMÀNTICA \n",
    "    print('Inici del càlcul de la similitud semàntica entre el document a analitzar i els documents de la categoria: '+start+label[0]+end)    \n",
    "    \n",
    "    path_txt_doc_analitzar = '../06FUNCIONAMENT_SISTEMA/txt_analitzar/*'\n",
    "    #Carrego el text del document a analitzar\n",
    "    llista_text_documents_plagiats = []\n",
    "    ###!!!1 MIRAR SI FUNCIONA BÉ EL PATH JA QUE ESTIC AGAFANT EL TXT GENERAT DE LA EXTRACCIÓ\n",
    "    for file in glob.glob(path_txt_doc_analitzar):\n",
    "        with open(file,'r') as txt:\n",
    "            llista_text_documents_plagiats.append(txt.read())\n",
    "    \n",
    "    #Extrec les oracions del document a analitzar\n",
    "    plag_document_sentences = []\n",
    "    for i in range (0,len(llista_text_documents_plagiats)):\n",
    "        plag_document_sentences.append(clean_raw_text(llista_text_documents_plagiats[i]))\n",
    "    \n",
    "   \n",
    "    #############################\n",
    "    if categoria =='Automobil':\n",
    "        \n",
    "        #LListo tots els tfg dins de la categoria automobil\n",
    "        llista_text_documents_automobil = []\n",
    "        for file in glob.glob('../00Data/dataset_txt/Automobil/*'):\n",
    "            with open(file,'r') as txt:\n",
    "                llista_text_documents_automobil.append(txt.read())\n",
    "                \n",
    "        #Pre-processo i separo en oracions els documents de la categoria automobil\n",
    "        base_document_sentences = []\n",
    "        for i in range (0,len(llista_text_documents_automobil)):\n",
    "            base_document_sentences.append(clean_raw_text(llista_text_documents_automobil[i]))\n",
    "\n",
    "        #Carrego el model word2vec entrenat amb el vocabulari de la categoria automobil  \n",
    "        with open('../03Models/w2v_models_for_compute_similarities/w2v_semantic_text_similarity_automobil.pkl', 'rb') as file:\n",
    "            w2v_model = pickle.load(file)\n",
    "        \n",
    "        #Creo una llista de llistes on tinc les diferents frases tokenitzades de cada document de la base de dades. [[[]]]\n",
    "        base_documents_frases_tokenitzades = []\n",
    "        for i in range (0,len(base_document_sentences)):\n",
    "            frases_tokenitzades = []\n",
    "            for sentence in base_document_sentences[i]:\n",
    "                frases_tokenitzades.append(nltk.word_tokenize(sentence))#Tokenitzo cada frase, obtenint una llista de paraules per a cada frase\n",
    "            base_documents_frases_tokenitzades.append(frases_tokenitzades)\n",
    "\n",
    "        #Creo una llista de llistes on tinc les diferents frases tokenitzades del document a analitzar.\n",
    "        plag_document_frases_tokenitzades = []\n",
    "        for sentence in plag_document_sentences[0]: \n",
    "            plag_document_frases_tokenitzades.append(nltk.word_tokenize(sentence))\n",
    "\n",
    "        #Filtro les paraules dels documents base de cada frase segons si pertanyen al vocabulari del word2vec\n",
    "        base_filtered_document_sentence_global = []\n",
    "        for document in base_documents_frases_tokenitzades: \n",
    "            filtered_sentence_global=[] \n",
    "            for sentence in document: \n",
    "                filtered_sentence_global.append([word for word in sentence if word in w2v_model.wv.vocab])\n",
    "            base_filtered_document_sentence_global.append(filtered_sentence_global)\n",
    "\n",
    "        #Filtro les paraules de les frases del document a analitzar segons si pertanyen al vocabulari del word2vec\n",
    "        plag_filtered_document_sentence = []\n",
    "        for sentence in plag_document_frases_tokenitzades:\n",
    "            plag_filtered_document_sentence.append([word for word in sentence if word in w2v_model.wv.vocab])\n",
    "            \n",
    "        #Transformo a vectors les paraules de cada frase de cada document de la base de dades a partir del wor2vec.\n",
    "        #No ha estat necessari filtrar les paraules perquè totes ja formen part del vocabulari. Si no fos així, hauria de filtrar-les \n",
    "        base_document_sentence_vectors = []\n",
    "        for i in range (0,len(base_filtered_document_sentence_global)):\n",
    "            frase_vectorizada = []\n",
    "            for sentence in base_filtered_document_sentence_global[i]:\n",
    "                mean = []\n",
    "                for word in sentence:\n",
    "                    mean.append(w2v_model.wv.get_vector(word))\n",
    "                mean = np.array(mean).mean(axis=0)\n",
    "                frase_vectorizada.append(mean)\n",
    "            base_document_sentence_vectors.append(frase_vectorizada)\n",
    "\n",
    "        #Transformo a vectors les paraules de cada frase del document a analitzar a partir del wor2vec.\n",
    "        plag_document_sentence_vectors = []\n",
    "        for sentence in plag_filtered_document_sentence:\n",
    "            mean = []\n",
    "            for word in sentence:\n",
    "                mean.append(w2v_model.wv.get_vector(word))\n",
    "            mean = np.array(mean).mean(axis=0)\n",
    "            plag_document_sentence_vectors.append(mean)\n",
    "            \n",
    "        #Transformo els vectors dels documents de la base de dades d'una dimensió a dues dimensions per tal de poder aplicar posteriorment cosine similarity\n",
    "        base_document_sentence_vectors_good_format = []\n",
    "        for document in base_document_sentence_vectors:\n",
    "            vectorizado_bueno = []\n",
    "            for vector in document:\n",
    "                vectorizado_bueno.append([vector])\n",
    "            base_document_sentence_vectors_good_format.append(vectorizado_bueno)\n",
    "            \n",
    "        #Transformo els vectors del document a analitzar d'una dimensió a dues dimensions per tal de poder aplicar posteriorment cosine similarity\n",
    "        plag_document_sentence_vectors_good_format = []\n",
    "        for vector in plag_document_sentence_vectors:\n",
    "            plag_document_sentence_vectors_good_format.append([vector])\n",
    "    \n",
    "        #Computo les similituds entre els vectors del document a analitzar i els vectors de cada document de la base de dades mitjançant cosine similarity\n",
    "        similarities_between_each_vector_plag_each_vector_base = []\n",
    "        for i in range (0,len(base_document_sentence_vectors_good_format)):\n",
    "            each_document = []\n",
    "            for vector2 in plag_document_sentence_vectors_good_format:\n",
    "                each_plag_sentence = []\n",
    "                for vector1 in base_document_sentence_vectors_good_format[i]:\n",
    "                    each_plag_sentence.append(cosine_similarity(vector2,vector1))\n",
    "                each_document.append(each_plag_sentence)\n",
    "            similarities_between_each_vector_plag_each_vector_base.append(each_document)    \n",
    "    \n",
    "        #Extrec les màximes similituds dels documents de la base de dades per a cada frase del document a analitzar\n",
    "        best_similarities_sentence_document = []\n",
    "        for i in range (0,len(similarities_between_each_vector_plag_each_vector_base)):\n",
    "            each_document = []\n",
    "            for each_vector in similarities_between_each_vector_plag_each_vector_base[i]:\n",
    "                each_document.append(max(each_vector))\n",
    "            best_similarities_sentence_document.append(each_document)\n",
    "\n",
    "        #Calculo l'index de similitud entre el document a analitzar i els documents de la base de dades\n",
    "        index_entre_documents_w2v = []\n",
    "        for each_document in best_similarities_sentence_document:\n",
    "            index_entre_documents_w2v.append(sum(each_document)/len(each_document))\n",
    "                \n",
    "        #Llindar de plagi fixat a partir de la distribució de freqüències de similitud entre oracions dels documents de la base de dades corresponents a la categoria automòbil\n",
    "        if max(index_entre_documents_w2v) > 0.35:\n",
    "            return('El document de la base de dades titulat: '+str(glob.glob('../00Data/dataset_txt/Automobil/*')[index_entre_documents_w2v.index(max(index_entre_documents_w2v))].split('/')[-1])+' te un percentatge de similitud del: '+str(max(index_entre_documents_w2v)[0])+' respecte al document a analitzar')\n",
    "    \n",
    "        else:\n",
    "            return('No s''ha detectat cap document a la base de dades que tingui una semblança major o igual al 35% i que pugui ser considerat objecte de plagi.')\n",
    "    #############################\n",
    "    if categoria =='Mecanica_fluids':\n",
    "        \n",
    "        #LListo tots els tfg dins de la categoria automobil\n",
    "        llista_text_documents_automobil = []\n",
    "        for file in glob.glob('../00Data/dataset_txt/Mecanica_fluids/*'):\n",
    "            with open(file,'r') as txt:\n",
    "                llista_text_documents_automobil.append(txt.read())\n",
    "                        \n",
    "        #Pre-processo i separo en oracions els documents de la categoria automobil\n",
    "        base_document_sentences = []\n",
    "        for i in range (0,len(llista_text_documents_automobil)):\n",
    "            base_document_sentences.append(clean_raw_text(llista_text_documents_automobil[i]))\n",
    "            \n",
    "        #Carrego el model word2vec entrenat amb el vocabulari de la categoria automobil  \n",
    "        with open('../03Models/w2v_models_for_compute_similarities/w2v_semantic_text_similarity_mecanica_fluids.pkl', 'rb') as file:\n",
    "            w2v_model = pickle.load(file)\n",
    "        \n",
    "        #Creo una llista de llistes on tinc les diferents frases tokenitzades de cada document de la base de dades. [[[]]]\n",
    "        base_documents_frases_tokenitzades = []\n",
    "        for i in range (0,len(base_document_sentences)):\n",
    "            frases_tokenitzades = []\n",
    "            for sentence in base_document_sentences[i]:\n",
    "                frases_tokenitzades.append(nltk.word_tokenize(sentence))#Tokenitzo cada frase, obtenint una llista de paraules per a cada frase\n",
    "            base_documents_frases_tokenitzades.append(frases_tokenitzades)\n",
    "\n",
    "        #Creo una llista de llistes on tinc les diferents frases tokenitzades del document a analitzar\n",
    "        plag_document_frases_tokenitzades = []\n",
    "        for sentence in plag_document_sentences[0]: \n",
    "            plag_document_frases_tokenitzades.append(nltk.word_tokenize(sentence))\n",
    "     \n",
    "        #Filtro les paraules dels documents base de cada frase segons si pertanyen al vocabulari del word2vec\n",
    "        base_filtered_document_sentence_global = []\n",
    "        for document in base_documents_frases_tokenitzades: \n",
    "            filtered_sentence_global=[] \n",
    "            for sentence in document: \n",
    "                filtered_sentence_global.append([word for word in sentence if word in w2v_model.wv.vocab])\n",
    "            base_filtered_document_sentence_global.append(filtered_sentence_global)\n",
    "\n",
    "        #Filtro les paraules del document a analitzar segons si pertanyen al vocabulari del word2vec\n",
    "        plag_filtered_document_sentence = []\n",
    "        for sentence in plag_document_frases_tokenitzades:\n",
    "            plag_filtered_document_sentence.append([word for word in sentence if word in w2v_model.wv.vocab])\n",
    " \n",
    "        #Transformo a vectors les paraules de cada frase de cada document de la base de dades a partir del wor2vec i genero un vector per a cada frase\n",
    "        #No ha estat necessari filtrar les paraules perquè totes ja formen part del vocabulari. Si no fos així, hauria de filtrar-les\n",
    "        base_document_sentence_vectors = []\n",
    "        for i in range (0,len(base_filtered_document_sentence_global)):\n",
    "            frase_vectorizada = []\n",
    "            for sentence in base_filtered_document_sentence_global[i]:\n",
    "                mean = []\n",
    "                for word in sentence:\n",
    "                    mean.append(w2v_model.wv.get_vector(word))\n",
    "                mean = np.array(mean).mean(axis=0)\n",
    "                frase_vectorizada.append(mean)\n",
    "            base_document_sentence_vectors.append(frase_vectorizada)\n",
    "\n",
    "        #Transformo a vectors les paraules de cada frase del document a analitzar a partir del word2vec i genero un vector per a cada frase\n",
    "        plag_document_sentence_vectors = []\n",
    "        for sentence in plag_filtered_document_sentence:\n",
    "            mean = []\n",
    "            for word in sentence:\n",
    "                mean.append(w2v_model.wv.get_vector(word))\n",
    "            mean = np.array(mean).mean(axis=0)\n",
    "            plag_document_sentence_vectors.append(mean)\n",
    "\n",
    "        #Transformo els vectors d'una dimensió a dues dimensions per tal de poder aplicar posteriorment cosine similarity\n",
    "        base_document_sentence_vectors_good_format = []\n",
    "        for document in base_document_sentence_vectors:\n",
    "            vectorizado_bueno = []\n",
    "            for vector in document:\n",
    "                vectorizado_bueno.append([vector])\n",
    "            base_document_sentence_vectors_good_format.append(vectorizado_bueno)\n",
    "\n",
    "        #Transformo els vectors del document a analitzar d'una dimensió a dues dimensions per tal de poder aplicar posteriorment cosine similarity\n",
    "        plag_document_sentence_vectors_good_format = []\n",
    "        for vector in plag_document_sentence_vectors:\n",
    "            plag_document_sentence_vectors_good_format.append([vector])\n",
    "            \n",
    "        #Computo les similituds entre els vectors del document a analitzar i els vectors de cada document de la base de dades mitjançant cosine similarity\n",
    "        similarities_between_each_vector_plag_each_vector_base = []\n",
    "        for i in range (0,len(base_document_sentence_vectors_good_format)):\n",
    "            each_document = []\n",
    "            for vector2 in plag_document_sentence_vectors_good_format:\n",
    "                each_plag_sentence = []\n",
    "                for vector1 in base_document_sentence_vectors_good_format[i]:\n",
    "                    each_plag_sentence.append(cosine_similarity(vector2,vector1))\n",
    "                each_document.append(each_plag_sentence)\n",
    "            similarities_between_each_vector_plag_each_vector_base.append(each_document)\n",
    "    \n",
    "        #Extrec les màximes similituds dels documents de la base de dades per a cada frase del document a analitzar\n",
    "        best_similarities_sentence_document = []\n",
    "        for i in range (0,len(similarities_between_each_vector_plag_each_vector_base)):\n",
    "            each_document = []\n",
    "            for each_vector in similarities_between_each_vector_plag_each_vector_base[i]:\n",
    "                each_document.append(max(each_vector))\n",
    "            best_similarities_sentence_document.append(each_document)\n",
    "\n",
    "        #Calculo l'index de similitud entre el document a analitzar i els documents de la base de dades de la categoria mecanica de fluids\n",
    "        index_entre_documents_w2v = []\n",
    "        for each_document in best_similarities_sentence_document:\n",
    "            index_entre_documents_w2v.append(sum(each_document)/len(each_document))\n",
    "              \n",
    "        if max(index_entre_documents_w2v) > 0.36: \n",
    "            return('El document de la base de dades titulat: '+str(glob.glob('../00Data/dataset_txt/Mecanica_fluids/*')[index_entre_documents_w2v.index(max(index_entre_documents_w2v))].split('/')[-1])+' te un percentatge de similitud del: '+str(max(index_entre_documents_w2v)[0])+' respecte al document a analitzar')\n",
    "        else:\n",
    "            return('No s''ha detectat cap document a la base de dades que tingui una semblança major o igual al 36% i que pugui ser considerat objecte de plagi.')  \n",
    "    #############################\n",
    "    if categoria =='Electronica':\n",
    "        \n",
    "        #LListo tots els tfg dins de la categoria automobil\n",
    "        llista_text_documents_automobil = []\n",
    "        for file in glob.glob('../00Data/dataset_txt/Electronica/*'):\n",
    "            with open(file,'r') as txt:\n",
    "                llista_text_documents_automobil.append(txt.read())\n",
    "                         \n",
    "        #Pre-processo i separo en oracions els documents de la categoria automobil\n",
    "        base_document_sentences = []\n",
    "        for i in range (0,len(llista_text_documents_automobil)):\n",
    "            base_document_sentences.append(clean_raw_text(llista_text_documents_automobil[i]))\n",
    "\n",
    "        #Carrego el model word2vec entrenat amb el vocabulari de la categoria automobil  \n",
    "        with open('../03Models/w2v_models_for_compute_similarities/w2v_semantic_text_similarity_electronica.pkl', 'rb') as file:\n",
    "            w2v_model = pickle.load(file)\n",
    "        \n",
    "        #Creo una llista de llistes de llistes on tinc les diferents frases tokenitzades de cada document de la base de dades. [[[]]]\n",
    "        base_documents_frases_tokenitzades = []\n",
    "        for i in range (0,len(base_document_sentences)):\n",
    "            frases_tokenitzades = []\n",
    "            for sentence in base_document_sentences[i]:\n",
    "                frases_tokenitzades.append(nltk.word_tokenize(sentence))#Tokenitzo cada frase, obtenint una llista de paraules per a cada frase\n",
    "            base_documents_frases_tokenitzades.append(frases_tokenitzades)\n",
    "\n",
    "        #Creo una llista de llistes on tinc les diferents frases tokenitzades del document a analitzar.\n",
    "        plag_document_frases_tokenitzades = []\n",
    "        for sentence in plag_document_sentences[0]: \n",
    "            plag_document_frases_tokenitzades.append(nltk.word_tokenize(sentence))\n",
    "\n",
    "        #Filtro les paraules dels documents base de cada frase segons si pertanyen al vocabulari del word2vec\n",
    "        base_filtered_document_sentence_global = []\n",
    "        for document in base_documents_frases_tokenitzades: \n",
    "            filtered_sentence_global=[] \n",
    "            for sentence in document: \n",
    "                filtered_sentence_global.append([word for word in sentence if word in w2v_model.wv.vocab])\n",
    "            base_filtered_document_sentence_global.append(filtered_sentence_global)\n",
    "\n",
    "        #Filtro les paraules del document a analitzar segons si pertanyen al vocabulari del word2vec\n",
    "        plag_filtered_document_sentence = []\n",
    "        for sentence in plag_document_frases_tokenitzades:\n",
    "            plag_filtered_document_sentence.append([word for word in sentence if word in w2v_model.wv.vocab])\n",
    "\n",
    "        #Transformo a vectors les paraules de cada frase de cada document a partir del wor2vec i genereo un vector per a cada frase.\n",
    "        #No ha estat necessari filtrar les paraules perquè totes ja formen part del vocabulari. Si no fos així, hauria de filtrar-les\n",
    "        base_document_sentence_vectors = []\n",
    "        for i in range (0,len(base_filtered_document_sentence_global)):\n",
    "            frase_vectorizada = []\n",
    "            for sentence in base_filtered_document_sentence_global[i]:\n",
    "                mean = []\n",
    "                for word in sentence:\n",
    "                    mean.append(w2v_model.wv.get_vector(word))\n",
    "                mean = np.array(mean).mean(axis=0)\n",
    "                frase_vectorizada.append(mean)\n",
    "            base_document_sentence_vectors.append(frase_vectorizada)\n",
    "\n",
    "\n",
    "        #Transformo a vectors les paraules de cada frase del document a partir del word2vec i genero un vector per a cada frase.\n",
    "        plag_document_sentence_vectors = []\n",
    "        for sentence in plag_filtered_document_sentence:\n",
    "            mean = []\n",
    "            for word in sentence:\n",
    "                mean.append(w2v_model.wv.get_vector(word))\n",
    "            mean = np.array(mean).mean(axis=0)\n",
    "            plag_document_sentence_vectors.append(mean)\n",
    "\n",
    "        #Transformo els vectors dels documents base d'una dimensió a dues dimensions per tal de poder aplicar posteriorment cosine similarity\n",
    "        base_document_sentence_vectors_good_format = []\n",
    "        for document in base_document_sentence_vectors:\n",
    "            vectorizado_bueno = []\n",
    "            for vector in document:\n",
    "                vectorizado_bueno.append([vector])\n",
    "            base_document_sentence_vectors_good_format.append(vectorizado_bueno)\n",
    "\n",
    "        #Transformo els vectors del document a analitzar d'una dimensió a dues dimensions per tal de poder aplicar posteriorment cosine similarity\n",
    "        plag_document_sentence_vectors_good_format = []\n",
    "        for vector in plag_document_sentence_vectors:\n",
    "            plag_document_sentence_vectors_good_format.append([vector])\n",
    "    \n",
    "        #Computo les similituds entre els vectors del document a analitzar i els vectors de cada document de la base de dades mitjançant cosine similarity\n",
    "        similarities_between_each_vector_plag_each_vector_base = []\n",
    "        for i in range (0,len(base_document_sentence_vectors_good_format)):\n",
    "            each_document = []\n",
    "            for vector2 in plag_document_sentence_vectors_good_format:\n",
    "                each_plag_sentence = []\n",
    "                for vector1 in base_document_sentence_vectors_good_format[i]:\n",
    "                    each_plag_sentence.append(cosine_similarity(vector2,vector1))\n",
    "                each_document.append(each_plag_sentence)\n",
    "            similarities_between_each_vector_plag_each_vector_base.append(each_document)\n",
    "    \n",
    "        #Extrec les màximes similituds dels documents de la base de dades per a cada frase del document a analitzar\n",
    "        best_similarities_sentence_document = []\n",
    "        for i in range (0,len(similarities_between_each_vector_plag_each_vector_base)):\n",
    "            each_document = []\n",
    "            for each_vector in similarities_between_each_vector_plag_each_vector_base[i]:\n",
    "                each_document.append(max(each_vector))\n",
    "            best_similarities_sentence_document.append(each_document)\n",
    "                  \n",
    "        #Calculo l'index de similitud entre el document a analitzar i els documents de la base de dades de la categoria electronica\n",
    "        index_entre_documents_w2v = []\n",
    "        for each_document in best_similarities_sentence_document:\n",
    "            index_entre_documents_w2v.append(sum(each_document)/len(each_document))\n",
    "        \n",
    "        if max(index_entre_documents_w2v)>0.44:   \n",
    "            return('El document de la base de dades titulat: '+str(glob.glob('../00Data/dataset_txt/Electronica/*')[index_entre_documents_w2v.index(max(index_entre_documents_w2v))].split('/')[-1])+' te un percentatge de similitud del: '+str(max(index_entre_documents_w2v)[0])+' respecte al document a analitzar')\n",
    "    \n",
    "        else:\n",
    "            return('No s''ha detectat cap document a la base de dades que tingui una semblança major o igual al 44% i que pugui ser considerat objecte de plagi.')                      \n",
    "    #############################\n",
    "                                             \n",
    "    if categoria =='Biomedicina':\n",
    "        \n",
    "        #LListo tots els tfg dins de la categoria biomedicina\n",
    "        llista_text_documents_automobil = []\n",
    "        for file in glob.glob('../00Data/dataset_txt/Biomedicina/*'):\n",
    "            with open(file,'r') as txt:\n",
    "                llista_text_documents_automobil.append(txt.read())\n",
    "         #Pre-processo i separo en oracions els documents de la categoria biomedicina\n",
    "        base_document_sentences = []\n",
    "        for i in range (0,len(llista_text_documents_automobil)):\n",
    "            base_document_sentences.append(clean_raw_text(llista_text_documents_automobil[i]))\n",
    "\n",
    "        #Carrego el model word2vec entrenat amb el vocabulari de la categoria biomedicina\n",
    "        with open('../03Models/w2v_models_for_compute_similarities/w2v_semantic_text_similarity_biomedicina.pkl', 'rb') as file:\n",
    "            w2v_model = pickle.load(file)\n",
    "        \n",
    "        #Creo una llista de llistes on tinc les diferents frases tokenitzades de cada document. [[[]]]\n",
    "        base_documents_frases_tokenitzades = []\n",
    "        for i in range (0,len(base_document_sentences)):\n",
    "            frases_tokenitzades = []\n",
    "            for sentence in base_document_sentences[i]:\n",
    "                frases_tokenitzades.append(nltk.word_tokenize(sentence))#Tokenitzo cada frase, obtenint una llista de paraules per a cada frase\n",
    "            base_documents_frases_tokenitzades.append(frases_tokenitzades)\n",
    "\n",
    "        #Creo una llista de llistes on tinc les diferents frases tokenitzades del document a analitzar\n",
    "        plag_document_frases_tokenitzades = []\n",
    "        for sentence in plag_document_sentences[0]: \n",
    "            plag_document_frases_tokenitzades.append(nltk.word_tokenize(sentence))\n",
    "\n",
    "        #Filtro les paraules dels documents base de cada frase segons si pertanyen al vocabulari del word2vec\n",
    "        base_filtered_document_sentence_global = []\n",
    "        for document in base_documents_frases_tokenitzades: \n",
    "            filtered_sentence_global=[] \n",
    "            for sentence in document: \n",
    "                filtered_sentence_global.append([word for word in sentence if word in w2v_model.wv.vocab])\n",
    "            base_filtered_document_sentence_global.append(filtered_sentence_global)\n",
    "\n",
    "        #Filtro les paraules del document plagiat segons si pertanyen al vocabulari del word2vec\n",
    "        plag_filtered_document_sentence = []\n",
    "        for sentence in plag_document_frases_tokenitzades:\n",
    "            plag_filtered_document_sentence.append([word for word in sentence if word in w2v_model.wv.vocab])\n",
    "\n",
    "        #Transformo a vectors les paraules de cada frase de cada document a partir del wor2vec.\n",
    "        #No ha estat necessari filtrar les paraules perquè totes ja formen part del vocabulari. Si no fos així, hauria de filtrar-les\n",
    "        base_document_sentence_vectors = []\n",
    "        for i in range (0,len(base_filtered_document_sentence_global)):\n",
    "            frase_vectorizada = []\n",
    "            for sentence in base_filtered_document_sentence_global[i]:\n",
    "                mean = []\n",
    "                for word in sentence:\n",
    "                    mean.append(w2v_model.wv.get_vector(word))\n",
    "                mean = np.array(mean).mean(axis=0)\n",
    "                frase_vectorizada.append(mean)\n",
    "            base_document_sentence_vectors.append(frase_vectorizada)\n",
    "\n",
    "        #Transformo les frases del document a analitzar a vectors a partir de la mitjana dels vectors de les paraules que conformen les frases\n",
    "        plag_document_sentence_vectors = []\n",
    "        for sentence in plag_filtered_document_sentence:\n",
    "            mean = []\n",
    "            for word in sentence:\n",
    "                mean.append(w2v_model.wv.get_vector(word))\n",
    "            mean = np.array(mean).mean(axis=0)\n",
    "            plag_document_sentence_vectors.append(mean)\n",
    "\n",
    "        #Transformo els vectors d'una dimensió a dues dimensions per tal de poder aplicar posteriorment cosine similarity\n",
    "        base_document_sentence_vectors_good_format = []\n",
    "        for document in base_document_sentence_vectors:\n",
    "            vectorizado_bueno = []\n",
    "            for vector in document:\n",
    "                vectorizado_bueno.append([vector])\n",
    "            base_document_sentence_vectors_good_format.append(vectorizado_bueno)\n",
    "\n",
    "        #Transformo els vectors d'una dimensió a dues dimensions per tal de poder aplicar posteriorment cosine similarity\n",
    "        plag_document_sentence_vectors_good_format = []\n",
    "        for vector in plag_document_sentence_vectors:\n",
    "            plag_document_sentence_vectors_good_format.append([vector])\n",
    "    \n",
    "        #Computo les similituds entre els vectors del document a analitzar i els vectors de cada document de la base de dades mitjançant cosine similarity\n",
    "        similarities_between_each_vector_plag_each_vector_base = []\n",
    "        for i in range (0,len(base_document_sentence_vectors_good_format)):\n",
    "            each_document = []\n",
    "            for vector2 in plag_document_sentence_vectors_good_format:\n",
    "                each_plag_sentence = []\n",
    "                for vector1 in base_document_sentence_vectors_good_format[i]:\n",
    "                    each_plag_sentence.append(cosine_similarity(vector2,vector1))\n",
    "                each_document.append(each_plag_sentence)\n",
    "            similarities_between_each_vector_plag_each_vector_base.append(each_document)\n",
    "    \n",
    "        #Extrec les màximes similituds dels documents de la base de dades per a cada frase del document a analitzar\n",
    "        best_similarities_sentence_document = []\n",
    "        for i in range (0,len(similarities_between_each_vector_plag_each_vector_base)):\n",
    "            each_document = []\n",
    "            for each_vector in similarities_between_each_vector_plag_each_vector_base[i]:\n",
    "                each_document.append(max(each_vector))\n",
    "            best_similarities_sentence_document.append(each_document)\n",
    "\n",
    "        #Calculo l'index de similitud entre el document a analitzar i els documents de la base de dades\n",
    "        index_entre_documents_w2v = []\n",
    "        for each_document in best_similarities_sentence_document:\n",
    "            index_entre_documents_w2v.append(sum(each_document)/len(each_document))\n",
    "            \n",
    "        \n",
    "    \n",
    "        \n",
    "        if max(index_entre_documents_w2v) > 0.38: #Canviar % del llindar que estableixi la dist freq de biomedicina\n",
    "            return('El document de la base de dades titulat: '+str(glob.glob('../00Data/dataset_txt/Biomedicina/*')[index_entre_documents_w2v.index(max(index_entre_documents_w2v))].split('/')[-1])+' te un percentatge de similitud del: '+str(max(index_entre_documents_w2v)[0])+' respecte al document a analitzar')\n",
    "    \n",
    "        else:\n",
    "            return('No s''ha detectat cap document a la base de dades que tingui una semblança major o igual al 38% i que pugui ser considerat objecte de plagi.')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arxiu ../06FUNCIONAMENT_SISTEMA/Arxiu_analitzar/La_implementacion_de_los_robots_electronicos_en_nuestras_casas.pdf pujat a storage amb el nom: La_implementacion_de_los_robots_electronicos_en_nuestras_casas.pdf. \n",
      "\n",
      "Extraient text del document mitjantçant Google API VISION. \n",
      "\n",
      "Descarregant arxiu: La_implementacion_de_los_robots_electronicos_en_nuestras_casas.pdfoutput-1-to-1.json localment. \n",
      "\n",
      "Arxiu La_implementacion_de_los_robots_electronicos_en_nuestras_casas.pdf eliminat.\n",
      "\n",
      "Arxiu La_implementacion_de_los_robots_electronicos_en_nuestras_casas.pdfoutput-1-to-1.json eliminat.\n",
      "\n",
      "Text extret a un arxiu .txt \n",
      "\n",
      "Inici del càlcul de la similitud semàntica entre el document a analitzar i els documents de la categoria: \u001b[1mElectronica\u001b[0;0m\n",
      "CPU times: user 1min 55s, sys: 12.6 s, total: 2min 7s\n",
      "Wall time: 2min 30s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'El document de la base de dades titulat: Estacion_robotizada_de_paletizado te un percentatge de similitud del: [0.5358954] respecte al document a analitzar'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "comparacio_doc_analitzar_vs_base_dades('../06FUNCIONAMENT_SISTEMA/Arxiu_analitzar/'+path.split('/')[-1],label[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neteja de les carpetes locals dels arxius a analitzar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Els arxius han estat moguts a les carpetes \"ANALITZATS\"\n"
     ]
    }
   ],
   "source": [
    "shutil.move('../06FUNCIONAMENT_SISTEMA/Arxiu_analitzar/'+path.split('/')[-1], '../06FUNCIONAMENT_SISTEMA/Arxius_analitzats/')\n",
    "if len(glob.glob('../06FUNCIONAMENT_SISTEMA/json_analitzar/*'))>=1:\n",
    "    shutil.move('../06FUNCIONAMENT_SISTEMA/json_analitzar/'+path.split('/')[-1].split('.')[0]+'.json', '../06FUNCIONAMENT_SISTEMA/jsons_analitzats/')\n",
    "shutil.move('../06FUNCIONAMENT_SISTEMA/txt_analitzar/'+path.split('/')[-1].split('.')[0]+'.txt', '../06FUNCIONAMENT_SISTEMA/txts_analitzats/')\n",
    "print('Els arxius han estat moguts a les carpetes \"ANALITZATS\"')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
